{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "498ff272",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd3df7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7110df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WideDeepRecommender: train on MIND _train then validate on _val and recommend\n",
    "# This cell contains a small, easy-to-follow pipeline. It tries to use pytorch_widedeep\n",
    "# if available; otherwise it falls back to a simple PyTorch logistic regression so\n",
    "# you can run the notebook without installing extra packages.\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Optional, List\n",
    "import json, ast\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class WideDeepRecommender:\n",
    "    def __init__(self, train_beh_path: str, val_beh_path: str, news_path: str, device: str = None):\n",
    "        self.train_beh_path = train_beh_path\n",
    "        self.val_beh_path = val_beh_path\n",
    "        self.news_path = news_path\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = None\n",
    "        self.preprocessor = None\n",
    "        self.news_df = None\n",
    "\n",
    "    # --- data loading utilities ---\n",
    "    def _detect_beh_columns(self, path: str):\n",
    "        with open(path, 'r', encoding='utf-8', errors='replace') as fh:\n",
    "            for line in fh:\n",
    "                if line.strip():\n",
    "                    parts = line.strip().split('\\t')\n",
    "                    return len(parts)\n",
    "        return 0\n",
    "\n",
    "    def load_behaviors(self, path: str, nrows: Optional[int] = None):\n",
    "        # supports both [impression_time, user_id, history, impressions]\n",
    "        # and [impression_id, user_id, impression_time, history, impressions]\n",
    "        ncols = self._detect_beh_columns(path)\n",
    "        if ncols >= 5:\n",
    "            names = ['impression_id','user_id','impression_time','history','impressions']\n",
    "        else:\n",
    "            names = ['impression_time','user_id','history','impressions']\n",
    "        df = pd.read_csv(path, sep='\\t', header=None, names=names, dtype=str, nrows=nrows, quoting=3)\n",
    "        # parse history\n",
    "        def parse_history(x):\n",
    "            if pd.isna(x) or str(x).strip() in ['', '-']:\n",
    "                return []\n",
    "            return str(x).split()\n",
    "        df['history'] = df.get('history', pd.Series(['']*len(df))).apply(parse_history)\n",
    "        df['raw_impression'] = df.get('impressions', pd.Series(['']*len(df))).fillna('')\n",
    "        # explode impressions into one row per (news_id,label)\n",
    "        rows = []\n",
    "        for _, r in df.iterrows():\n",
    "            raw = str(r['raw_impression']).strip()\n",
    "            if raw == '':\n",
    "                rows.append({'user_id': r.get('user_id', None), 'news_id': None, 'label': None})\n",
    "                continue\n",
    "            for token in raw.split():\n",
    "                if '-' in token:\n",
    "                    news, lbl = token.rsplit('-',1)\n",
    "                    try:\n",
    "                        lbl = int(lbl)\n",
    "                    except Exception:\n",
    "                        lbl = None\n",
    "                else:\n",
    "                    news, lbl = token, None\n",
    "                rows.append({'user_id': r.get('user_id', None), 'news_id': news, 'label': lbl})\n",
    "        return pd.DataFrame(rows)\n",
    "\n",
    "    def load_news(self, path: str, nrows: Optional[int] = None):\n",
    "        col_names = ['news_id','category','subcategory','title','abstract','url','entities','concepts']\n",
    "        df = pd.read_csv(path, sep='\\t', header=None, names=col_names, nrows=nrows, dtype=str, quoting=3)\n",
    "        def safe_parse(s):\n",
    "            if pd.isna(s) or str(s).strip() == '':\n",
    "                return []\n",
    "            try:\n",
    "                return json.loads(s)\n",
    "            except Exception:\n",
    "                try:\n",
    "                    return ast.literal_eval(s)\n",
    "                except Exception:\n",
    "                    return []\n",
    "        df['entities_parsed'] = df['entities'].apply(safe_parse)\n",
    "        def extract_labels(x):\n",
    "            if isinstance(x, list):\n",
    "                return [it.get('Label') for it in x if isinstance(it, dict) and it.get('Label')]\n",
    "            return []\n",
    "        df['entity_labels'] = df['entities_parsed'].apply(extract_labels)\n",
    "        df['title_abstract'] = df['title'].fillna('') + ' ' + df['abstract'].fillna('')\n",
    "        self.news_df = df\n",
    "        return df\n",
    "\n",
    "    # --- prepare train/val dataframes of interactions ---\n",
    "    def prepare_interactions(self, nrows_train: Optional[int] = None, nrows_val: Optional[int] = None):\n",
    "        train_df = self.load_behaviors(self.train_beh_path, nrows=nrows_train)\n",
    "        val_df = self.load_behaviors(self.val_beh_path, nrows=nrows_val)\n",
    "        self.load_news(self.news_path)\n",
    "        # drop null news_id rows\n",
    "        train_df = train_df[train_df['news_id'].notna()].copy()\n",
    "        val_df = val_df[val_df['news_id'].notna()].copy()\n",
    "        # keep only interactions where news exists in news_df\n",
    "        valid_news = set(self.news_df['news_id'])\n",
    "        train_df = train_df[train_df['news_id'].isin(valid_news)]\n",
    "        val_df = val_df[val_df['news_id'].isin(valid_news)]\n",
    "        # convert label column to int\n",
    "        train_df['label'] = train_df['label'].astype('Int64').fillna(0).astype(int)\n",
    "        val_df['label'] = val_df['label'].astype('Int64').fillna(0).astype(int)\n",
    "        self.train_inter = train_df.reset_index(drop=True)\n",
    "        self.val_inter = val_df.reset_index(drop=True)\n",
    "        return self.train_inter, self.val_inter\n",
    "\n",
    "    # --- train using pytorch_widedeep if available, otherwise fallback ---\n",
    "    def train(self, epochs: int = 3, batch_size: int = 512, lr: float = 1e-3):\n",
    "        # prepare interactions if not ready\n",
    "        if getattr(self, 'train_inter', None) is None:\n",
    "            self.prepare_interactions()\n",
    "\n",
    "        try:\n",
    "            # Try to import pytorch_widedeep and build a simple Wide + TabMlp model\n",
    "            from pytorch_widedeep.preprocessing import TabPreprocessor, WidePreprocessor\n",
    "            from pytorch_widedeep.models import Wide, TabMlp, WideDeep\n",
    "            from pytorch_widedeep import Trainer\n",
    "\n",
    "            # prepare categorical columns\n",
    "            X_train = self.train_inter[['user_id','news_id']].astype(str)\n",
    "            X_val = self.val_inter[['user_id','news_id']].astype(str)\n",
    "            y_train = self.train_inter['label'].values\n",
    "            y_val = self.val_inter['label'].values\n",
    "\n",
    "            # Tab preprocessor: embed both user_id and news_id\n",
    "            tab_preprocessor = TabPreprocessor(embed_cols=['user_id','news_id'], continuous_cols=[])\n",
    "            X_tab_train = tab_preprocessor.fit_transform(X_train)\n",
    "            X_tab_val = tab_preprocessor.transform(X_val)\n",
    "\n",
    "            wide_preprocessor = WidePreprocessor(categorical_cols=['user_id','news_id'])\n",
    "            X_wide_train = wide_preprocessor.fit_transform(X_train)\n",
    "            X_wide_val = wide_preprocessor.transform(X_val)\n",
    "\n",
    "            wide = Wide(wide_dim=X_wide_train.shape[1])\n",
    "            deeptabular = TabMlp(mlp_hidden_dims=[64,32], dropout=0.2)\n",
    "            model = WideDeep(wide=wide, deeptabular=deeptabular)\n",
    "\n",
    "            trainer = Trainer(model, objective='binary', metrics=['auc','accuracy'], use_cuda=(self.device=='cuda'))\n",
    "            trainer.fit(X_tab=X_tab_train, X_wide=X_wide_train, target=y_train,\n",
    "                        X_tab_val=X_tab_val, X_wide_val=X_wide_val, val_target=y_val,\n",
    "                        n_epochs=epochs, batch_size=batch_size, lr=lr)\n",
    "\n",
    "            self.model = model\n",
    "            self.preprocessor = {'tab': tab_preprocessor, 'wide': wide_preprocessor}\n",
    "            print('Trained pytorch_widedeep model.')\n",
    "            return True\n",
    "\n",
    "        except Exception as e:\n",
    "            print('pytorch_widedeep training failed or not available, falling back to a simple PyTorch model.')\n",
    "            print('Debug info:', e)\n",
    "            return self._train_fallback(epochs=epochs, batch_size=batch_size, lr=lr)\n",
    "\n",
    "    def _train_fallback(self, epochs=3, batch_size=512, lr=1e-3):\n",
    "        # Build simple one-hot features for user_id and news_id (may be large). For demo, we'll\n",
    "        # encode user_id and news_id into integer indices and learn embeddings with a small NN.\n",
    "        train = self.train_inter\n",
    "        val = self.val_inter\n",
    "        users = pd.concat([train['user_id'], val['user_id']]).unique()\n",
    "        news = self.news_df['news_id'].unique()\n",
    "        user2idx = {u:i for i,u in enumerate(users)}\n",
    "        news2idx = {n:i for i,n in enumerate(news)}\n",
    "        train['u_idx'] = train['user_id'].map(user2idx)\n",
    "        train['n_idx'] = train['news_id'].map(news2idx)\n",
    "        val['u_idx'] = val['user_id'].map(user2idx).fillna(-1).astype(int)\n",
    "        val['n_idx'] = val['news_id'].map(news2idx).fillna(-1).astype(int)\n",
    "\n",
    "        n_users = len(user2idx)\n",
    "        n_news = len(news2idx)\n",
    "        emb_dim = 32\n",
    "\n",
    "        class SimpleRecModel(nn.Module):\n",
    "            def __init__(self, n_users, n_news, emb_dim):\n",
    "                super().__init__()\n",
    "                self.u_emb = nn.Embedding(n_users, emb_dim)\n",
    "                self.n_emb = nn.Embedding(n_news, emb_dim)\n",
    "                self.out = nn.Linear(emb_dim*2, 1)\n",
    "            def forward(self, u_idx, n_idx):\n",
    "                u = self.u_emb(u_idx)\n",
    "                n = self.n_emb(n_idx)\n",
    "                x = torch.cat([u,n], dim=1)\n",
    "                return self.out(x).squeeze(1)\n",
    "\n",
    "        model = SimpleRecModel(n_users, n_news, emb_dim).to(self.device)\n",
    "        loss_fn = nn.BCEWithLogitsLoss()\n",
    "        opt = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "        # create tensors\n",
    "        import torch\n",
    "        u_train = torch.tensor(train['u_idx'].values, dtype=torch.long, device=self.device)\n",
    "        n_train = torch.tensor(train['n_idx'].values, dtype=torch.long, device=self.device)\n",
    "        y_train = torch.tensor(train['label'].values, dtype=torch.float32, device=self.device)\n",
    "        u_val = torch.tensor(val['u_idx'].values, dtype=torch.long, device=self.device)\n",
    "        n_val = torch.tensor(val['n_idx'].values, dtype=torch.long, device=self.device)\n",
    "        y_val = torch.tensor(val['label'].values, dtype=torch.float32, device=self.device)\n",
    "\n",
    "        # simple training loop\n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            opt.zero_grad()\n",
    "            logits = model(u_train, n_train)\n",
    "            loss = loss_fn(logits, y_train)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            # val\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_logits = model(u_val, n_val)\n",
    "                val_loss = loss_fn(val_logits, y_val)\n",
    "                # compute simple accuracy\n",
    "                preds = (torch.sigmoid(val_logits) > 0.5).float()\n",
    "                acc = (preds == y_val).float().mean().item()\n",
    "            print(f'Epoch {epoch+1}/{epochs} train_loss={loss.item():.4f} val_loss={val_loss.item():.4f} val_acc={acc:.4f}')\n",
    "\n",
    "        # store fitted objects\n",
    "        self.model = model\n",
    "        self.preprocessor = {'user2idx': user2idx, 'news2idx': news2idx}\n",
    "        self._fallback_news_index = news2idx\n",
    "        self._fallback_user_index = user2idx\n",
    "        self._is_fallback = True\n",
    "        return True\n",
    "\n",
    "    # --- recommendation ---\n",
    "    def recommend_for_user(self, user_id: str, top_k: int = 10) -> List[dict]:\n",
    "        # If widedeep model trained, use it; otherwise use fallback model\n",
    "        if getattr(self, '_is_fallback', False):\n",
    "            # map user to index\n",
    "            u2i = self.preprocessor['user2idx']\n",
    "            n2i = self.preprocessor['news2idx']\n",
    "            if user_id not in u2i:\n",
    "                print('Unknown user, returning most popular news')\n",
    "                # popularity fallback\n",
    "                popular = (self.train_inter['news_id'].value_counts().index[:top_k].tolist())\n",
    "                return [{'news_id': nid, 'title': self.news_df.loc[self.news_df.news_id==nid,'title'].iloc[0]} for nid in popular]\n",
    "            ui = torch.tensor([u2i[user_id]], dtype=torch.long, device=self.device)\n",
    "            # score all news\n",
    "            inv_news = {v:k for k,v in n2i.items()}\n",
    "            all_n_idx = torch.tensor(list(range(len(n2i))), dtype=torch.long, device=self.device)\n",
    "            self.model.eval()\n",
    "            with torch.no_grad():\n",
    "                logits = self.model(ui.repeat(len(all_n_idx)), all_n_idx)\n",
    "                scores = torch.sigmoid(logits).cpu().numpy()\n",
    "            top_idx = scores.argsort()[::-1][:top_k]\n",
    "            recs = []\n",
    "            for idx in top_idx:\n",
    "                nid = inv_news[int(all_n_idx[idx].item())]\n",
    "                title = self.news_df.loc[self.news_df.news_id==nid, 'title'].iloc[0]\n",
    "                recs.append({'news_id': nid, 'title': title, 'score': float(scores[idx])})\n",
    "            return recs\n",
    "        else:\n",
    "            # Try to use pytorch_widedeep predict API (best-effort)\n",
    "            try:\n",
    "                # prepare a DataFrame with the user and all candidate news\n",
    "                cand = pd.DataFrame({'user_id': [user_id]*len(self.news_df), 'news_id': self.news_df['news_id'].tolist()})\n",
    "                tab = self.preprocessor['tab'].transform(cand)\n",
    "                wide = self.preprocessor['wide'].transform(cand)\n",
    "                # depending on the trained Trainer API you may need to call model.predict_proba\n",
    "                from pytorch_widedeep.utils import predict\n",
    "                preds = predict(self.model, X_tab=tab, X_wide=wide)\n",
    "                self.news_df['score'] = preds\n",
    "                top = self.news_df.sort_values('score', ascending=False).head(top_k)\n",
    "                return top[['news_id','title','score']].to_dict('records')\n",
    "            except Exception as e:\n",
    "                print('Failed to use widedeep predict path:', e)\n",
    "                return []\n",
    "\n",
    "# Usage example (not executed here):\n",
    "# train_path = '/workspace/data/MIND_small/MINDsmall_train/behaviors.tsv'\n",
    "# val_path = '/workspace/data/MIND_small/MINDsmall_val/behaviors.tsv'\n",
    "# news_path = '/workspace/data/MIND_small/MINDsmall_train/news.tsv'\n",
    "# rec = WideDeepRecommender(train_path, val_path, news_path)\n",
    "# rec.prepare_interactions(nrows_train=50000, nrows_val=5000)  # adjust sizes for memory\n",
    "# rec.train(epochs=3)\n",
    "# print(rec.recommend_for_user('U13740', top_k=5))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
